{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spellchk: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Read `answer/default.py` starting with the `spellchk` function and see how it solves the task of spell correction using a pre-trained language model that can predict a replacement token for a masked token in the input.\n",
    "\n",
    "In your submission, write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\tit will put your mind into non-stop learning.\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "with StringIO(\"4\\tit will put your maind into non-stop learning.\") as f:\n",
    "    for (locations, spellchk_sent) in spellchk(f):\n",
    "        print(\"{locs}\\t{sent}\".format(\n",
    "            locs=\",\".join([str(i) for i in locations]),\n",
    "            sent=\" \".join(spellchk_sent)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "## 1. Exploration and Understanding\n",
    "\n",
    "+ To further understand the intermediate output of the codes, I changed the original functions in the default.py to generate the test output.\n",
    "+ Also, since I never used transformer.pipeline's fill-mask package before, I go to the official documentation: https://huggingface.co/docs/transformers/task_summary#masked-language-modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ get_typo_locations fnction parse the input test cases and generates lists of typo locations and the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([5, 14],), ['Just', 'before', 'Myra', 'left', '--', 'Sue', 'was', 'saying', 'good-by', 'to', 'Cathy', ',', 'and', 'she', \"didm't\", 'realize', 'I', 'was', 'near', \"''\", '.']]\n"
     ]
    }
   ],
   "source": [
    "fill_mask = pipeline('fill-mask', model='distilbert-base-uncased')\n",
    "mask = fill_mask.tokenizer.mask_token\n",
    "\n",
    "exp1 = \"5,14\tJust before Myra left -- Sue was saying good-by to Cathy , and she didm't realize I was near '' .\"\n",
    "\n",
    "def get_typo_locations(sentence):\n",
    "    tsv_f = [sentence.split('\\t')]\n",
    "    for line in tsv_f:\n",
    "            # line[0] contains the comma separated indices of typo words\n",
    "        first = [int(i) for i in line[0].split(',')],\n",
    "            # line[1] contains the space separated tokens of the sentence\n",
    "        second = line[1].split()\n",
    "    return [first, second]\n",
    "\n",
    "res1 = get_typo_locations(exp1)\n",
    "print(res1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The core function inside spellchk function is the fill_mask provided by transformer.pipeline which could take in a masked sentence and generate suggestions soley by the context of the input sentence.\n",
    "+ The returned sugguestion words are ranked by descending scores, so the default solution diretly returned the top scored word recommended by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.10249210149049759, 'token': 18305, 'token_str': 'cathy', 'sequence': \"just before myra left - - cathy was saying good - by to cathy, and she didm't realize i was near''.\"}, {'score': 0.046036895364522934, 'token': 23020, 'token_str': 'myra', 'sequence': \"just before myra left - - myra was saying good - by to cathy, and she didm't realize i was near''.\"}, {'score': 0.041393160820007324, 'token': 2016, 'token_str': 'she', 'sequence': \"just before myra left - - she was saying good - by to cathy, and she didm't realize i was near''.\"}, {'score': 0.03479192033410072, 'token': 9056, 'token_str': 'liz', 'sequence': \"just before myra left - - liz was saying good - by to cathy, and she didm't realize i was near''.\"}, {'score': 0.031346630305051804, 'token': 12954, 'token_str': 'mum', 'sequence': \"just before myra left - - mum was saying good - by to cathy, and she didm't realize i was near''.\"}]\n",
      "[{'score': 0.8567774891853333, 'token': 2106, 'token_str': 'did', 'sequence': \"just before myra left - - sue was saying good - by to cathy, and she did realize i was near''.\"}, {'score': 0.054612815380096436, 'token': 2052, 'token_str': 'would', 'sequence': \"just before myra left - - sue was saying good - by to cathy, and she would realize i was near''.\"}, {'score': 0.02943958155810833, 'token': 2071, 'token_str': 'could', 'sequence': \"just before myra left - - sue was saying good - by to cathy, and she could realize i was near''.\"}, {'score': 0.01944577880203724, 'token': 2453, 'token_str': 'might', 'sequence': \"just before myra left - - sue was saying good - by to cathy, and she might realize i was near''.\"}, {'score': 0.009886316023766994, 'token': 2442, 'token_str': 'must', 'sequence': \"just before myra left - - sue was saying good - by to cathy, and she must realize i was near''.\"}]\n"
     ]
    }
   ],
   "source": [
    "def select_correction(typo, predict):\n",
    "    # return the most likely prediction for the mask token\n",
    "    return predict[0]['token_str']\n",
    "\n",
    "res1 = [[5, 14], ['Just', 'before', 'Myra', 'left', '--', 'Sue', 'was', 'saying', 'good-by', 'to', 'Cathy', ',', 'and', 'she', \"didm't\", 'realize', 'I', 'was', 'near', \"''\", '.']]\n",
    "\n",
    "locations, sent = res1\n",
    "\n",
    "for i in locations:\n",
    "    predict = fill_mask(\n",
    "                    \" \".join([ sent[j] if j != i else mask for j in range(len(sent)) ]), # replace typo with a mask\n",
    "                    top_k=5\n",
    "                )\n",
    "    print(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Improvement Solution 1 : Edit Distance\n",
    "+ From the above explorations, I found out the default solution miss out one important information, that is the miss-spelling word itself. Though there are some characteristics spelled wrong, most characters are in the right places. So it would be better to use this additional information, and pick out the most similar word from the top 20 suggested words, the correction accuracy should be better.\n",
    "+ Another hint in the default solution is, the select_correction function has an input named 'typo', but it is never used.\n",
    "\n",
    "Considering the above thought, We decided to select the suggested word with the smallest edit distance with the typo. This increase the the score from 0.23 to 0.52."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typo: Sue\n",
      "she\n",
      "typo: didm't\n",
      "did\n"
     ]
    }
   ],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "res1 = [[5, 14], ['Just', 'before', 'Myra', 'left', '--', 'Sue', 'was', 'saying', 'good-by', 'to', 'Cathy', ',', 'and', 'she', \"didm't\", 'realize', 'I', 'was', 'near', \"''\", '.']]\n",
    "\n",
    "locations, sent = res1\n",
    "\n",
    "predict = fill_mask(\n",
    "            \" \".join([ sent[j] if j != 5 else mask for j in range(len(sent)) ]), # replace typo with a mask\n",
    "            top_k=20\n",
    "        )\n",
    "\n",
    "def select_correction(typo, predict):\n",
    "    recommended_words = [p['token_str'] for p in predict]\n",
    "    levenshtein_distances = [distance(typo, word) for word in recommended_words]\n",
    "    index = levenshtein_distances.index(min(levenshtein_distances))\n",
    "    print(predict[index]['token_str'])\n",
    "\n",
    "print(\"typo:\", res1[1][5])\n",
    "select_correction('Sue', predict)\n",
    "\n",
    "predict = fill_mask(\n",
    "            \" \".join([ sent[j] if j != 14 else mask for j in range(len(sent)) ]), # replace typo with a mask\n",
    "            top_k=20\n",
    "        )\n",
    "\n",
    "print(\"typo:\", res1[1][14])\n",
    "select_correction('didm\\'t', predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After considering both the edit distance and context, the dev.out score comes to 0.52, which is much better than the default score of 0.23. But still, it is not that satisfying. So let's check the output inference to understand some of the wrongly fixed cases. \n",
    "+ We noticed that the top_k variable limits the number of recommend words, that is if this number is too small, the correct word will not be returned by the API. So we increased the top_k variable to see what happended. \n",
    "After we changed the top_k from 20 to 1000, the score raise from 0.52 to 0.68."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Improvement Solution 2: Filter the 0 Distance out & Optimize the Selection of Distance\n",
    "\n",
    "When we further modified our model, we discovered that some words do not need to be modified during the process, i.e., their distance is 0, resulting in a large number of unnecessary replacement procedures. Therefore, we utilized the filter function to exclude words with a distance of 0 and increase the model's performance.\n",
    "\n",
    "We sorted the data after applying filters, prioritizing predictions depending on how close they are. First, the distance's size was considered. When the distances were equal, the scores were sorted to see which had the highest score. This strategy greatly enhanced our model's overall performance. Using the previously given methods, we further increased the top_k value to 3000, resulting in an overall score of 0.70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_correction(typo, predict):\n",
    "    # calculate the edit distance between typo and token_str\n",
    "    predict = [{**p, 'ldis': distance(typo, p['token_str'])} for p in predict]\n",
    "    # keep the predict if the distance is not 0\n",
    "    filter_predict = list(filter(filter_p, predict))\n",
    "    # sort the predict and select the closer distance\n",
    "    # if the distances are equal, then choose predict with the higher score\n",
    "    sort_predict = sorted(filter_predict, key=sorting_k)\n",
    "    return sort_predict[0]['token_str']\n",
    "\n",
    "def filter_p(p):\n",
    "    return p['ldis'] != 0\n",
    "\n",
    "def sorting_k(p):\n",
    "    res = (p['ldis'], -p['score'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Improvement Solution 3: Capital Letter in the front of the Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By obseving some outputs generated, we found out that some wrong predictions are becase upper/lower case matching. When the typo words are at the first word of a sentence, the first letter of the word should be upper case. But our previous solution does not handle this case. Thus, we add another few lines of code in the spellchk function to process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellchk(fh):\n",
    "\tfor (locations, sent) in get_typo_locations(fh):\n",
    "\t\t# some codes ...\n",
    "\t\t# Added a condition to change the case of the first character of the word when the index of the word being updated is 0 i.e it is the first word of the sentence.             \n",
    "\t\tcorrect_word = select_correction(sent[i], predict)\n",
    "\t\tif (i==0):\n",
    "\t\t\tcorrect_word= correct_word.capitalize()\n",
    "\t\tspellchk_sent[i]=correct_word\n",
    "\t\t# some code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group work\n",
    "\n",
    "* ywa422: Proposed and implemented Method 1.\n",
    "* ningyik: Proposed and implemented Method 2.\n",
    "* asa489: Proposed and implemented Method 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
