{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prefixtune: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiwenw/cmpt713/nlpclass-1241-g-Geranimus/hw4/prefixtune/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from default import *\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:13,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0||  _______________________________________________    A new report from the US Department of Justice's Office of the Inspector General (OIG) has revealed that the Justice Department's Office of the Inspector General (OIG) has found that the Department of Justice has\n",
      "1||   The following is a list of the most popular and popular websites in the United States.    The following is a list of the most popular and popular websites in the United States.    The following is a list of the\n",
      "2||  __________   If you're looking for the first time in your life, you're looking for the first time in your life. If you're looking for the first time in your life, you're looking for the first time in your life\n",
      "3||  ___________   A new report from the U.S. Department of Homeland Security (DHS) in the wake of the 9/11 attacks on the U.S. and its allies in the Middle East has revealed that the U.\n",
      "4||  __________________________________________    I’ve been working on a new project for the past few months, and I’ve been working on a new project for the past few months, and I’ve been working on a new\n",
      "5||  _______________________________________________   In the past few months, I’ve been waiting for a new blog post to come out. In the past few months, I’ve been waiting for a new blog post to come out. In the\n",
      "6||  _______________________________________________    I have been working on a new project for the last few years, and I have been working on a new project for the last few years, and I have been working on a new project for the last few years\n",
      "7||  iced coffee is good for you.    The first thing you need to know is what type of coffee is available in the United States.   The first thing you need to know is what type of coffee is available in the United\n",
      "8||  ____________________________________________________________________   This is the first in a series of articles on the importance of the internet and the internet. This article is part of the series of articles on the importance of the internet. This article is part of the series of articles on\n",
      "9||  ___________    The first thing you need to know is that you are the first to know how to use the app on your Android device.  The first thing you need to know is that you are the first to know how to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "basemodel = 'distilgpt2'\n",
    "table_to_text = TableToText(\"peft\", basemodel=basemodel)\n",
    "model = AutoModelForCausalLM.from_pretrained(basemodel)\n",
    "decoder_output = table_to_text.decode(model, '../data/input/small.txt')\n",
    "print(\"\\n\".join(decoder_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the warnings from the transformers library. They are expected to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `bleu.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prefix Tunning\n",
    "Prefix Tunning is already implemented and wrapped up by huggingface, we study the usage of the peft by browsing the offical tutorial: https://huggingface.co/docs/peft/quicktour. And we altered the codes provided by the official insturction according to our needs.\n",
    "\n",
    "+ First we need to design the prefix tunning configurations. Since we are doing a causal inference task with distilgpt2, we choose the `task_type` to be CAUSAL_LM. And we need to set the `inference_mode` to be False, and then set the predesigned `num_virtual_tokens` and `prefix_projection`. (Notice that in the prefix_projection is set to False in default, but here we need it to be True).\n",
    "+ Then we conbine the prefix tunning component with the distilgpt2 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = PrefixTuningConfig(task_type=TaskType.CAUSAL_LM,\n",
    "\t\t\t\t\t\t\t\t\tinference_mode=False,\n",
    "\t\t\t\t\t\t\t\t\tnum_virtual_tokens=self.virtualtokens, \n",
    "\t\t\t\t\t\t\t\t\tprefix_projection=self.prefixprojection)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ During the training loop, we need to update the prefix related weights, thus in each epoch, we need to perform the backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(self.epochs):\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\t# TODO rest of the training steps for prefix tuning\n",
    "\t\tfor key in data_loaders.keys():\n",
    "\t\t\tfor step, batch in enumerate(tqdm(data_loaders[key])):\n",
    "\t\t\t\tbatch = {k: v.to(device) for k, v in batch.items()}\n",
    "\t\t\t\toutputs = model(**batch)\n",
    "\t\t\t\tloss = outputs.loss\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\tlr_scheduler.step()\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In the inference part, we need to load the prefix tunning layers and the base model together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "        print(f\"Loading the non-finetuned pre-trained model: {opts.basemodel}\", file=sys.stderr)\n",
    "        model = AutoModelForCausalLM.from_pretrained(opts.basemodel)\n",
    "        model = model.to(device)\n",
    "else:\n",
    "\tif not os.path.isdir(modelfile + opts.modelsuffix) or opts.force:\n",
    "\t\tprint(f\"Could not find modelfile {modelfile + opts.modelsuffix} or -f used. Starting training.\", file=sys.stderr)\n",
    "\t\ttable_to_text.train()\n",
    "\t\tprint(\"Training done.\", file=sys.stderr)\n",
    "\t# use the model file if available and opts.force is False\n",
    "\tassert(os.path.isdir(modelfile + opts.modelsuffix))\n",
    "\tprint(f\"Found modelfile {modelfile + opts.modelsuffix}. Starting decoding.\", file=sys.stderr)\n",
    "\t# TODO: if using hf peft library for prefix tuning:\n",
    "\n",
    "\tmodel = AutoModelForCausalLM.from_pretrained(opts.basemodel)\n",
    "\tmodel = PeftModel.from_pretrained(model, modelfile + opts.modelsuffix)\n",
    "\tmodel = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the basic setings with no paramameter changing(compared to default file), the BLEU scores are:  \n",
    "+ small.out score: 13.4207\n",
    "+ dev.out score: 16.1767"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Tunning\n",
    "Other than the basic settings of the prefix tunning component, there is also lots of hyperparameters that we can adjust.\n",
    "### 2.1 Preft - Number of Virtual Tokens\n",
    "The number of virtual token controls the length of the embeddings added to each input to the model. Theoretically, when the number is too small, it is not enough to encode the task-related information, and lead to relatively low performace. The feault number of virtual token is 5, we tried to increase it, while leaving all other hyperparameter to be the same.\n",
    "+ **virtualtokens = 10**: small.out score: 15.5933, dev.out score: 18.1553\n",
    "+ **virtualtokens = 20**:  small.out score: 19.2093, dev.out score: 17.9956\n",
    "\n",
    "### 2.2 Decoding - max_new_tokens\n",
    "The max_new_tokens controls the max length of the sentence generated by the model. The default solution uses 50.\n",
    "\n",
    "### 2.3 Decoding - num_beams\n",
    "The num_beams controls the beam search width. The default solution uses 5.\n",
    "\n",
    "### 2.5 Decoding - temperature\n",
    "The temperature controls the next token possibility, that is the lower the tempreature, the smaller the softmax outputs.\n",
    "\n",
    "+ **virtualtokens = 20, max_new_tokens = 50, num_beams = 3, temperature = 0.1**: small.out score: 16.0280, dev.out score: 14.3532\n",
    "+ **virtualtokens = 20, max_new_tokens = 20, num_beams = 7, temperature = 0.2**: dev.small.out score: 11.0139, out score: 12.4284\n",
    "+ **virtualtokens = 5, max_new_tokens = 20, num_beams = 7, temperature = 0.2**: small.out score: 28.8313, dev.out score: 21.1418\n",
    "+ **virtualtokens = 5, max_new_tokens = 20, num_beams = 5, temperature = 0.2**: small.out score: 30.3896, dev.out score: 30.3138"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
